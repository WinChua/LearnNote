# 特征工程处理

[TOC]

## 什么是特征工程

​	数据和特征决定了机器学习的上限，模型和算法逼近这个上限而已。而特征工程就是从源数据中最大限度的提取特征以提供算法和模型使用。

​	![特征工程的技能图](tzgc.jpg)

## 数据预处理

​	在我们通过某种方式提取了特征之后，这些未经过处理 的特征可能会存在下面的问题：

* 量纲不一样
* 信息冗余： 对于某些定量特征，器包含的有效信息为区间划分，例如学习成绩， 假如只关心及格和不及格，我们可以使用二值化对该问题进行处理
* 定性特征不能直接使用， 某些机器学习算法和模型只能接受定量特横的输入， 那么需要将定性特征转换为定量特征，最简单的方式是为每一种定性值指定一种定量值，但是这种方式过于灵活，增加调参的难度。通常使用onehot编码，将一个特征拓展为n个特征
* 存在缺失值： 对缺失值需要使用某个默认值进行补充
* 信息利用率低： 

上述问题可以使用sklearn 中的preprocessing来进行处理

### 无量纲化

​	无量纲化是指不同规格之间的数据转换到统一规格的数据，常用的有两种， 标准化和 区间缩放法。

​	标准化的前提是： 元数据服从正态分布

​	区间缩放： 利用边界信息，将特征的取值区间缩放放到某个特定的范围，比如 [0, 1]

#### 标准化

​	使用的公式是：



​									$$x' = \frac{x - E(X)}{Std(x))}$$

​	上述公式使用的前提是，数据服从正太分布。在sklearn中使用preprocessing中的StandardScaler进行标准化处理

```python
from sklearn.preprocessing import StandardScaler
std = StandardScaler()
std.fit_transform(iris.data)
```

#### 区间缩放法

​	使用的公式是

​									$$x' = \frac{x - Min}{Max - Min}$$

​	

```python
from sklearn.preprocessing import MinMaxScaler
mmScaler = MinMaxScaler().fit_transform(iris.data)
```



#### 标准化与归一化的区别

​	标准化是对列数据进行处理，使得列数据的分布符合$N(0,1)$ 的标准正太分布

​	归一化是对行数据进行处理，使得每一个数据的长度都为1 , 其目的在于 样本向量再点乘运算或者其他和函数计算相似性的时候，拥有统一的标准，归一化的公式为:

​									$$ x' = \frac{x}{\sqrt{\sum_{j}{x[j]}^2}}$$

​	

```python
from sklearn.preprocessing import Normalizer
Normalizer().fit_transform(iris.data)
```



### 对定量特征二值化

​	二值化的核心在于，设定一个阈值，将大于阈值的数据设置为1， 小于阈值的数据设置为0：


$$
x' = \left\{
\begin{array}{rcl}
1, x \gt thresold\\
0, x \lt thresold
\end{array}
\right.
$$
​	

````python
from sklearn.preprocessing import Binarizer
Binarizer(threshold = 3).fit_transform(iris.data)
````

### 对定性特征进行onehot编码

```python
from sklearn.preprocessing import OneHotEncoder
OneHotEncoder().fit_transform(iris.target.reshape((-1, 1)))
```



### 缺失值计算

​	使用 Imputer 对数据的缺失值进行处理

```python
from numpy import vstack, array, nan
from sklear.preprocessing import Imputer
Imputer().fit_transform(vstack((array([nan, nan, nan, nan]), iris.data)))
```



### 数据变换

​	常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。对于4个特征， 度为2 的多项式转换公式如下：
$$


(x'_1, x'_2, x'_3, x'_4, x'_5, x'_6, x'_7, x'_8, x'_9, x'_{10}, x'_{11}, x'_{12}, x'_{13}, x'_{14}, x'_{15}, x'_{16}) = x_i * x_j, i = [1,2,3,4], j = [1,2,3,4]


$$

```python
from sklearn.preprocessing import PolynomialFeatures
PolynomialFeatures().fit_transform(iris.data)
```

​	对于单变元函数的数据变换可以使用一个统一的方式完成，使用preprocessing库的FunctionTransformer 对数据进行函数转换：

```python
from numpy import log1p
from sklearn.preprocessing import FunctionTransformer
FunctionTransformer(log1p).fit_transform(iris.data)
```



### 回顾





## 特征选择

当数据预处理完成之后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练，通常来说， 从两个方面考虑来选择特征：

* 特征是否发散： 如果一个特征不发散，例如方差接近于0， 也就是说样本在这个特征上面没有区分性
* 特征于目标的相关性： 这点比较显而易见， 于目标相关性高的特征，应当有选选择，除了方差之外，其他指标都是从相关性上面进行考虑的。

根据特征选择的形式又可以将特征选择方法分为三种：

* filter: 过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者带选择预知的个数， 选择特征
* wrapper： 包装法， 根据目标函数， 每次选择若干特征， 或者排除若干特征。
* Embedded： 嵌入法， 先试用某些机器学习的算法或者模型进行训练， 得到各个特征的全职系数， 根据系数从大到小选择特征。 类似于filer方法， 但是通过训练来确定特征的优略。

我们使用 sklearn 中的feature_selection 来进行特征选择。

### filter

#### 方差选择法

​	使用方差选择法，先要计算各个特征的方差， 然后根据与之，选择呢方差大于阈值的特征。使用feature_selection 库中的WarianceThreshold 类进行特征的选择， 如下:

```python
from sklearn.feature_selection import VarianceThreshold
VarianceThreshold(threshold = 3).fit_transform(iris.data)
```

#### 相关系数法

​	使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。使用feature_selection 库的SlectKBest 类结合相关系数来选择特征的代码如下：

```pyhon
from sklearn.feature_selection import SelectKBest
from scipy.stats import pearsonr
SelectKBest(lambda X, Y: array(map(lambda x: pearsonr(x, Y), X.T)).T, k = 2).fit_transform(iris.data, iris.target)
```

#### 卡方检验

​	卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值， 因变量有M种取值， 考虑自变量等于i切因变量等于j的样本频数的观察值于期望的差距，构建统计量：
$$
x^2 = \sum\frac{(A - E)^2}{E}
$$

```python
from sklearn.feature_selection ikmport SelectKBest
from sklearn.feature_selection import chi2
SelectKBest(chi2, k = 2).fit_transform(iris.data, iris.target)
```

#### 互信息法

​	户信息发用于评价定性自变量对定性因变量的相关性， 计算公式：
$$
I(X;Y) = \sum_{x \in X}\sum_{y \in Y}p(x, y)log{\frac{p(x, y)}{p(x)p(y)}}
$$

```python
from sklearn.feature_selection import SelectKBest
from minepy import MINE
def mic(x, y):
    m = MINE()
    m.compute_score(x, y)
    return (m.mic(), 0.5)
SelectKBest(lambda X, Y: array(map(lambda x: mic(x, Y), X.T)).T, k = 2).fit_transform(iris.data, iris.target)
```



### wrapper

#### 递归特征消除法

使用一个基模型来进行多轮训练， 每轮训练后，消除若干权值系数的特征，在基于新的特征及进行下一轮的训练，使用feature_selection 中的RFE来选择特征的代码如下：

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
RFE(estimator = LogisticRegression(), n_features_to_select = 2).fit_transform(iris.data, iris.target)
```



### Embedded(惩罚)

#### 基于惩罚项的特征选择法

​	使用带惩罚项的基模型， 除了筛选出特征之外，同时也进行了将为。使用feature_selection 库种的SelectFromModel 类结合带 L1 惩罚项的逻辑回归模型，来选择特征的代码如下:

```python
from sklearn.feature_selection import SelectFromModel
from sklearn.lineart_model import LogisticRegression
SelectFromModel(LogisticRegression(penalty  = "l1", C = 0.1)).fit_transform(iris.data, iris.target)
```

#### 基于树模型的特征选择法



## 降维

​	当特征选择完成了之后，可以直接进行模型训练，但是可能由于特征矩阵过大（比如，所使用onehot编码等将会直接将特征扩展）需要降维，一般来说有两种方法， PCA和LDA方法，这两个方法都是要将原始数据样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样： PCA是使得数据的维度具有最大的发散性，也就是说，方差最大的几个方向；而LDA为了映射后的数据具有更好的分类性能

### PCA

### LDA

### 回顾



## 总结

## 参考资料

1.  [what's dummy coding?](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/dummy.htm)
2. [卡方检验](http://wiki.mbalib.com/wiki/%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C)
3. [干货：结合Scikit-learn介绍几种常用的特征选择方法](http://dataunion.org/14072.html)
4. [机器学习中，有哪些特征选择的工程方法？](http://www.zhihu.com/question/28641663/answer/41653367)
5. [机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)](http://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html)

