#  机器学习杂记

1. 监督学习，往简单的讲就是要在最小化损失的同时，规则化参数项，如下公式：

   ​
   $$
   \omega^* = arg \underset{\omega}{min}\sum L(y_i;f(x_i;\omega)) + \lambda\Omega(\omega)
   $$

2. 大多数带参数的模型都和这个形似，同时神似。大部分的就是在对这两个项进行变换

   * 如果损失函数是 Square Lose， 就是最小二乘法
   * Hinge Loss ，就是SVM
   * exp-Loss ， 就是 Boosting
   * Log-Loss， 就是 Logistic Regression

3. 对于第二项，规则化函数$\Omega(\omega)$ ， 也具有多种选择，一般是模型复杂度的单调递增函数， 模型越复杂，规则化值越大。通常我们会选择规则化项是模型参数响亮的范数。然而，不同的选择对参数$\omega$ 曰素不同。常见的，我们使用 0范数，一阶范数， 二姐范数，迹范数，核范数。

   * L0范数， $\omega$ 中非零元素的个数， 非凸函数， 最小化该范数，将会使得$\omega$ 变得稀疏，模型更加简单
   * L1范数， $\omega$ 中各个元素绝对值之和，或者叫做曼哈顿距离， L1 是 L0 范数的最优凸近似。

4. 为什么要稀疏？
   因为他能对特征进行自动选择。，一般来说， $x_i$ 的大部分元素都是和最终的输出 $y_i$ 没有关系的或者不提供任何信息的，在最小化目标函数的时候考虑$x_i$ 这些额外的特征，虽然可以获得更小的训练误差，但是在预测新的样本的时候，这些没有用的信息反而会被考虑， 从而干扰了对正确 $y_i$ 的预测。稀疏规则化算子将会吧无效特征对应的权重值 置零，相当于删除该特征

5. L2范数
   除了L1范数，还有一种范数  L2， 其实就是$\omega$ 向量的欧氏距离，有人将L2 称为 "岭回归"或者"权值衰减"。最主要的问题是他能够改善机器学习中的过拟合问题。为什么L2能够防止过拟合？
   由于，L2范数是欧氏距离， 在最小化欧氏距离的时候， 我们可以使每一个参数都很小， 接近于0，但是于L1范数不同的是，不会直接让参数等于0。 越小的参数说明模型越简单， 越简单的模型则约不容易产生过拟合现象。为什么月晓得参数说明模型越简单？多项式的某些项对于预测来说是重要的，或者说某个特征是重要的，可以通过调整该项的权重，调小，而不是像L1一样直接使参数变为0，

6. ​

